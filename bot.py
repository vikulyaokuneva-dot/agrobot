import json
import os
import feedparser
import requests
import asyncio
from bs4 import BeautifulSoup
from telegram import Bot

from rss_sources import RSS_SOURCES

print("üî• BOT.PY LOADED üî•")

TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = "@helpgardener"
STORAGE_FILE = "storage.json"

EMOJIS = ["üå±", "ü™¥", "üåº", "üåø", "üçÉ"]
HASHTAGS = "#—Å–∞–¥ #–æ–≥–æ—Ä–æ–¥ #–¥–∞—á–∞"

SERIES_RULES = {
    "ü•î –ù–µ–¥–µ–ª—è –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è": ["–∫–∞—Ä—Ç–æ—Ñ", "–∫–ª—É–±–Ω"],
    "üå± –í—Å—ë –æ —Ä–∞—Å—Å–∞–¥–µ": ["—Ä–∞—Å—Å–∞–¥", "—Å–µ—è–Ω"],
    "üåø –ë–æ–ª–µ–∑–Ω–∏ —Ä–∞—Å—Ç–µ–Ω–∏–π": ["–±–æ–ª–µ–∑–Ω", "–≥–Ω–∏–ª—å", "–ø—è—Ç–Ω"],
    "ü™¥ –ü–æ–ª–∏–≤ –±–µ–∑ –æ—à–∏–±–æ–∫": ["–ø–æ–ª–∏–≤", "–≤–ª–∞–≥"],
    "üì¶ –•—Ä–∞–Ω–µ–Ω–∏–µ —É—Ä–æ–∂–∞—è": ["—Ö—Ä–∞–Ω–µ–Ω", "–ø–æ–≥—Ä–µ–±", "–ø–æ–¥–≤–∞–ª"],
}

SEASON_RULES = {
    "üå± –í–µ—Å–µ–Ω–Ω–∏–µ —Ä–∞–±–æ—Ç—ã": {
        "months": [3, 4, 5],
        "keywords": ["—Ä–∞—Å—Å–∞–¥", "–ø–æ—Å–∞–¥", "–≥—Ä—è–¥", "–ø–æ—á–≤"]
    },
    "‚òÄÔ∏è –õ–µ—Ç–Ω–∏–π —É—Ö–æ–¥": {
        "months": [6, 7, 8],
        "keywords": ["–ø–æ–ª–∏–≤", "–≤—Ä–µ–¥", "–±–æ–ª–µ–∑–Ω", "–ø–æ–¥–∫–æ—Ä–º"]
    },
    "üçÇ –û—Å–µ–Ω–Ω–∏–π —É—Ä–æ–∂–∞–π": {
        "months": [9, 10, 11],
        "keywords": ["—É—Ä–æ–∂–∞", "—Ö—Ä–∞–Ω–µ–Ω", "—É–±–æ—Ä–∫", "–æ–±—Ä–µ–∑"]
    },
    "‚ùÑÔ∏è –ó–∏–º–Ω–∏–µ —Å–æ–≤–µ—Ç—ã": {
        "months": [12, 1, 2],
        "keywords": ["–∫–æ–º–Ω–∞—Ç", "–∑–∏–º", "—Ö—Ä–∞–Ω–µ–Ω", "–ø–ª–∞–Ω"]
    }
}


# ---------- STORAGE ----------

def load_storage():
    if not os.path.exists(STORAGE_FILE):
        return {"posts_count": 0, "published_links": {}}

    with open(STORAGE_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    # –º–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
    if "published_links" not in data:
        return {
            "posts_count": len(data),
            "published_links": data
        }

    return data


def save_storage(data):
    with open(STORAGE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def increment_posts_count(storage):
    storage["posts_count"] = storage.get("posts_count", 0) + 1


def should_make_short_post(posts_count):
    return posts_count != 0 and posts_count % 10 == 0


# ---------- SERIES ----------

def detect_series(title, text):
    combined = f"{title} {text}".lower()
    for name, keywords in SERIES_RULES.items():
        for kw in keywords:
            if kw in combined:
                return name
    return None

from datetime import datetime

def detect_season_series(title, text):
    month = datetime.now().month
    combined = f"{title} {text}".lower()

    for season, rule in SEASON_RULES.items():
        if month not in rule["months"]:
            continue

        for kw in rule["keywords"]:
            if kw in combined:
                return season

    return None


# ---------- CONTENT ----------

def clean_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ").strip()


def extract_image(entry):
    if "media_content" in entry:
        media = entry.media_content
        if media and media[0].get("url"):
            return media[0]["url"]

    if "enclosures" in entry and entry.enclosures:
        enc = entry.enclosures
        if enc and enc[0].get("href"):
            return enc[0]["href"]

    soup = BeautifulSoup(entry.get("description", ""), "html.parser")
    img = soup.find("img")
    if img and img.get("src"):
        return img["src"]

    try:
        response = requests.get(
            entry.link,
            timeout=10,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        page = BeautifulSoup(response.text, "html.parser")
        og = page.find("meta", property="og:image")
        if og and og.get("content"):
            return og["content"]
    except Exception:
        pass

    return None


def extract_full_text(url):
    try:
        response = requests.get(
            url,
            timeout=10,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        soup = BeautifulSoup(response.text, "html.parser")

        for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "aside"]):
            tag.decompose()

        paragraphs = soup.find_all("p")
        text = "\n".join(
            p.get_text(strip=True)
            for p in paragraphs
            if len(p.get_text(strip=True)) > 40
        )

        return text[:4000]
    except Exception:
        return ""


def summarize_text(text):
    sentences = text.split(".")
    bullets = []

    for s in sentences:
        s = s.strip()
        if 50 < len(s) < 200:
            bullets.append(f"‚Ä¢ {s}")
        if len(bullets) >= 5:
            break

    return "\n".join(bullets)


def get_latest_news(storage):
    for source in RSS_SOURCES:
        feed = feedparser.parse(source)

        for entry in feed.entries:
            link = entry.get("link")
            if not link or link in storage["published_links"]:
                continue

            image = extract_image(entry)
            if not image:
                continue

            title = clean_html(entry.get("title", ""))

            full_text = extract_full_text(link)
            summary = summarize_text(full_text)
            if not summary:
                continue

            series = detect_series(title, summary) or detect_season_series(title, summary)


            return {
                "title": title,
                "description": summary,
                "link": link,
                "image": image,
                "series": series
            }

    return None


# ---------- POSTING ----------

def generate_short_post():
    tips = [
        "–ù–µ –ø–æ–ª–∏–≤–∞–π—Ç–µ —Ä–∞—Å—Ç–µ–Ω–∏—è —Ö–æ–ª–æ–¥–Ω–æ–π –≤–æ–¥–æ–π ‚Äî —ç—Ç–æ —Å—Ç—Ä–µ—Å—Å –¥–ª—è –∫–æ—Ä–Ω–µ–π.",
        "–õ—É—á—à–µ –Ω–µ–¥–æ–ª–∏—Ç—å —Ä–∞—Å—Ç–µ–Ω–∏–µ, —á–µ–º –ø–µ—Ä–µ–ª–∏—Ç—å.",
        "–†—ã—Ö–ª–µ–Ω–∏–µ –ø–æ—á–≤—ã –ø–æ—Å–ª–µ –ø–æ–ª–∏–≤–∞ —É–ª—É—á—à–∞–µ—Ç –¥–æ—Å—Ç—É–ø –∫–∏—Å–ª–æ—Ä–æ–¥–∞.",
        "–ù–µ —Å–∞–∂–∞–π—Ç–µ —Ä–∞—Å—Å–∞–¥—É –≤ —Ö–æ–ª–æ–¥–Ω—É—é –∑–µ–º–ª—é ‚Äî —Ä–æ—Å—Ç –∑–∞–º–µ–¥–ª–∏—Ç—Å—è.",
        "–ü–æ–∂–µ–ª—Ç–µ–Ω–∏–µ –ª–∏—Å—Ç—å–µ–≤ —á–∞—Å—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –ø–µ—Ä–µ—É–≤–ª–∞–∂–Ω–µ–Ω–∏–∏."
    ]
    return "üå± *–°–æ–≤–µ—Ç –¥–Ω—è*\n\n" + tips[hash(os.urandom(4)) % len(tips)]


async def post_short(text):
    bot = Bot(token=TOKEN)
    await bot.send_message(chat_id=CHAT_ID, text=text, parse_mode="Markdown")


async def post_full(news):
    bot = Bot(token=TOKEN)
    emoji = EMOJIS[hash(news["title"]) % len(EMOJIS)]

    series_block = f"{news['series']}\n\n" if news.get("series") else ""

    caption = (
        f"{series_block}"
        f"{emoji} *{news['title']}*\n\n"
        f"{news['description']}\n\n"
        f"‚úçÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {news['link']}\n\n"
        f"{HASHTAGS}"
    )

    await bot.send_photo(
        chat_id=CHAT_ID,
        photo=news["image"],
        caption=caption,
        parse_mode="Markdown"
    )


# ---------- MAIN ----------

def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")

    if not TOKEN:
        print("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    storage = load_storage()
    print(f"üì¶ –ü–æ—Å—Ç–æ–≤ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {storage['posts_count']}")

    if should_make_short_post(storage["posts_count"]):
        print("üìù –ö–æ—Ä–æ—Ç–∫–∏–π –ø–æ—Å—Ç")
        asyncio.run(post_short(generate_short_post()))
        increment_posts_count(storage)
        save_storage(storage)
        return

    news = get_latest_news(storage)
    if not news:
        print("‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return

    asyncio.run(post_full(news))
    storage["published_links"][news["link"]] = True
    increment_posts_count(storage)
    save_storage(storage)

    print("‚úÖ –ü–æ—Å—Ç –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω")


if __name__ == "__main__":
    main()
