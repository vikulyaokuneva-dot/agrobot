import json
import os
import feedparser
import requests
import asyncio
from bs4 import BeautifulSoup
from telegram import Bot

from rss_sources import RSS_SOURCES

SERIES_RULES = {
    "ü•î –ù–µ–¥–µ–ª—è –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è": ["–∫–∞—Ä—Ç–æ—Ñ", "–∫–ª—É–±–Ω"],
    "üå± –í—Å—ë –æ —Ä–∞—Å—Å–∞–¥–µ": ["—Ä–∞—Å—Å–∞–¥", "—Å–µ—è–Ω"],
    "üåø –ë–æ–ª–µ–∑–Ω–∏ —Ä–∞—Å—Ç–µ–Ω–∏–π": ["–±–æ–ª–µ–∑–Ω", "–≥–Ω–∏–ª—å", "–ø—è—Ç–Ω"],
    "ü™¥ –ü–æ–ª–∏–≤ –±–µ–∑ –æ—à–∏–±–æ–∫": ["–ø–æ–ª–∏–≤", "–≤–ª–∞–≥"],
    "üì¶ –•—Ä–∞–Ω–µ–Ω–∏–µ —É—Ä–æ–∂–∞—è": ["—Ö—Ä–∞–Ω–µ–Ω", "–ø–æ–≥—Ä–µ–±", "–ø–æ–¥–≤–∞–ª"]
}

print("üî• BOT.PY LOADED üî•")

TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = "@helpgardener"
STORAGE_FILE = "storage.json"

EMOJIS = ["üå±", "ü™¥", "üåº", "üåø", "üçÉ"]
HASHTAGS = "#—Å–∞–¥ #–æ–≥–æ—Ä–æ–¥ #–¥–∞—á–∞"

def detect_series(title, text):
    combined = f"{title} {text}".lower()

    for series_name, keywords in SERIES_RULES.items():
        for kw in keywords:
            if kw in combined:
                return series_name

    return None


def load_storage():
    if not os.path.exists(STORAGE_FILE):
        return {}
    with open(STORAGE_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_storage(data):
    with open(STORAGE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def clean_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text(separator=" ").strip()


def extract_image(entry):
    # 1. media:content
    if "media_content" in entry:
        media = entry.media_content
        if media and media[0].get("url"):
            return media[0]["url"]

    # 2. enclosure
    if "enclosures" in entry and entry.enclosures:
        enc = entry.enclosures
        if enc and enc[0].get("href"):
            return enc[0]["href"]

    # 3. img –≤ description
    soup = BeautifulSoup(entry.get("description", ""), "html.parser")
    img = soup.find("img")
    if img and img.get("src"):
        return img["src"]

    # 4. og:image —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏ (fallback)
    try:
        print("üîç –ò—â—É og:image –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å—Ç–∞—Ç—å–∏")
        response = requests.get(
            entry.link,
            timeout=10,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        page = BeautifulSoup(response.text, "html.parser")
        og = page.find("meta", property="og:image")
        if og and og.get("content"):
            print(f"üñº –ù–∞–π–¥–µ–Ω–æ og:image: {og['content']}")
            return og["content"]
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {e}")

    return None


def extract_full_text(url):
    try:
        response = requests.get(
            url,
            timeout=10,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        soup = BeautifulSoup(response.text, "html.parser")

        # —É–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä
        for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "aside"]):
            tag.decompose()

        paragraphs = soup.find_all("p")
        text = "\n".join(
            p.get_text(strip=True)
            for p in paragraphs
            if len(p.get_text(strip=True)) > 40
        )

        return text[:4000]
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏: {e}")
        return ""


def summarize_text(text):
    sentences = text.split(".")
    bullets = []

    for s in sentences:
        s = s.strip()
        if 50 < len(s) < 200:
            bullets.append(f"‚Ä¢ {s}")

        if len(bullets) >= 5:
            break

    return "\n".join(bullets)


def get_latest_news():
    storage = load_storage()

    for source in RSS_SOURCES:
        feed = feedparser.parse(source)

        for entry in feed.entries:
            link = entry.get("link")
            if not link or link in storage:
                continue

            image = extract_image(entry)
            if not image:
                continue

            title = clean_html(entry.get("title", ""))

            full_text = extract_full_text(link)
            summary = summarize_text(full_text)
            series = detect_series(title, summary)

            if not summary:
                continue

            return {
                "title": title,
                "description": summary,
                "link": link,
                "image": image,
                "series": series
            }

    return None


async def post_to_telegram(news):
    bot = Bot(token=TOKEN)

    emoji = EMOJIS[hash(news["title"]) % len(EMOJIS)]
    series_block = ""
    if news.get("series"):
        series_block = f"{news['series']}\n\n"

    caption = (
        f"{series_block}"
        f"{emoji} *{news['title']}*\n\n"
        f"{news['description']}\n\n"
        f"‚úçÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {news['link']}\n\n"
        f"{HASHTAGS}"
    )


    await bot.send_photo(
        chat_id=CHAT_ID,
        photo=news["image"],
        caption=caption,
        parse_mode="Markdown"
    )


def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")

    if not TOKEN:
        print("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    storage = load_storage()
    print(f"üì¶ –í storage –∑–∞–ø–∏—Å–µ–π: {len(storage)}")

    news = get_latest_news()

    if not news:
        print("‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
        return

    print(f"üì∞ –ù–∞–π–¥–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å: {news['title']}")
    print(f"üñº –ö–∞—Ä—Ç–∏–Ω–∫–∞: {news['image']}")

    asyncio.run(post_to_telegram(news))

    storage[news["link"]] = True
    save_storage(storage)

    print("‚úÖ –ù–æ–≤–æ—Å—Ç—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞")


if __name__ == "__main__":
    main()
