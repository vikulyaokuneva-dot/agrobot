# parsers.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- –§—É–Ω–∫—Ü–∏–∏ –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø —Å—Å—ã–ª–æ–∫ ---

def discover_supersadovnik_links(soup, base_url):
    links = set()
    for a in soup.find_all('a', class_='item-post-common__title'):
        if a.has_attr('href'):
            links.add(urljoin(base_url, a['href']))
    return list(links)

def discover_botanichka_links(soup, base_url):
    links = set()
    for h2 in soup.find_all('h2', class_='post-title'):
        a = h2.find('a')
        if a and a.has_attr('href'):
            links.add(urljoin(base_url, a['href']))
    return list(links)
    
def discover_ogorod_ru_links(soup, base_url):
    links = set()
    for a in soup.select('.item-article .item-title a, .rubric-popular-item a'):
        if a.has_attr('href'):
             links.add(urljoin(base_url, a['href']))
    return list(links)
    
def discover_dolinadad_links(soup, base_url):
    links = set()
    for a in soup.find_all('a', class_='blog-item__title-link'):
        if a.has_attr('href'):
             links.add(urljoin(base_url, a['href']))
    return list(links)
    
def discover_tk_konstruktor_links(soup, base_url):
    links = set()
    for a in soup.select('.post-item .post-title a'):
        if a.has_attr('href'):
             links.add(urljoin(base_url, a['href']))
    return list(links)


# --- –§—É–Ω–∫—Ü–∏–∏ –ü–ê–†–°–ò–ù–ì–ê –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π ---

def parse_supersadovnik(soup):
    title = soup.find('h1').get_text(strip=True)
    content_div = soup.find('div', class_='article__text')
    paragraphs = content_div.find_all(['p', 'h2', 'h3'])
    content = '\n\n'.join(p.get_text(strip=True) for p in paragraphs)
    return title, content

def parse_botanichka(soup):
    title = soup.find('h1', class_='post-title').get_text(strip=True)
    content_div = soup.find('div', class_='post-content')
    if content_div.find('div', class_='read-also'):
        content_div.find('div', class_='read-also').decompose()
    paragraphs = content_div.find_all(['p', 'h2', 'h3', 'li'])
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs).replace('\n', '\n\n')
    return title, content

def parse_ogorod_ru(soup):
    title = soup.find('h1').get_text(strip=True)
    content_div = soup.find('div', class_='article-body-content-inner')
    paragraphs = content_div.find_all(['p', 'h2', 'h3', 'li'])
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs).replace('\n', '\n\n')
    return title, content

def parse_dolinadad(soup):
    title = soup.find('h1', class_='blog-post__title').get_text(strip=True)
    content_div = soup.find('div', class_='blog-post__content')
    paragraphs = content_div.find_all(['p', 'h2', 'h3', 'li'])
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs).replace('\n', '\n\n')
    return title, content
    
def parse_tk_konstruktor(soup):
    title = soup.find('h1').get_text(strip=True)
    content_div = soup.find('div', class_='post-content')
    paragraphs = content_div.find_all(['p', 'h2', 'h3', 'li'])
    content = '\n'.join(p.get_text(strip=True) for p in paragraphs).replace('\n', '\n\n')
    return title, content


# --- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä—ã ---

def get_html_soup(url):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è '—Å—É–ø–∞' —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    response = requests.get(url, headers=headers, timeout=15)
    response.raise_for_status()
    return BeautifulSoup(response.text, 'html.parser')


def discover_new_articles(target_url):
    """–î–∏—Å–ø–µ—Ç—á–µ—Ä –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: –≤—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–π discover-–ø–∞—Ä—Å–µ—Ä."""
    print(f"  –°–∫–∞–Ω–∏—Ä—É—é {target_url}...")
    soup = get_html_soup(target_url)
    
    if 'supersadovnik.ru' in target_url:
        return discover_supersadovnik_links(soup, target_url)
    elif 'botanichka.ru' in target_url:
        return discover_botanichka_links(soup, target_url)
    elif 'ogorod.ru' in target_url:
        return discover_ogorod_ru_links(soup, target_url)
    elif 'dolinasad.by' in target_url:
        return discover_dolinadad_links(soup, target_url)
    elif 'tk-konstruktor.ru' in target_url:
        return discover_tk_konstruktor_links(soup, target_url)
    return []


def parse_article(url):
    """–î–∏—Å–ø–µ—Ç—á–µ—Ä –ø–∞—Ä—Å–∏–Ω–≥–∞: –≤—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–π parse-–ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å—Ç–∞—Ç—å–∏."""
    try:
        soup = get_html_soup(url)
        
        if 'supersadovnik.ru' in url:
            title, content = parse_supersadovnik(soup)
        elif 'botanichka.ru' in url:
            title, content = parse_botanichka(soup)
        elif 'ogorod.ru' in url:
            title, content = parse_ogorod_ru(soup)
        elif 'dolinasad.by' in url:
            title, content = parse_dolinadad(soup)
        elif 'tk-konstruktor.ru' in url:
            title, content = parse_tk_konstruktor(soup)
        else:
            return None, "–≠—Ç–æ—Ç —Å–∞–π—Ç –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç–µ–π. üòï"
        
        formatted_message = f"**{title.strip()}**\n\n{content.strip()}\n\n[–ò—Å—Ç–æ—á–Ω–∏–∫]({url})"
        return formatted_message, None
        
    except Exception as e:
        return None, f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}."
